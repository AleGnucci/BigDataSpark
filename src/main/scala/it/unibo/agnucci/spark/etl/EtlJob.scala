package it.unibo.agnucci.spark.etl

import java.text.{ParseException, SimpleDateFormat}
import org.apache.spark.sql.types.BooleanType
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.functions.{to_date,unix_timestamp}

object EtlJob {

  //spark and sc are already available in spark shell
  private val spark = SparkSession
    .builder()
    .appName("Etl Phase")
    .getOrCreate()
  private val sc = spark.sparkContext
  private val TRENDINGDATEFORMAT = "yy.dd.MM"
  private val PUBLISHTIMEFORMAT = "yyyy-MM-dd'T'HH:mm:ss.SSSX"

  import spark.implicits._

  def main(args: Array[String]): Unit = {

    val basePath = "hdfs:/user/agnucci/datasets/youtube-new/"
    val CAdf = readRequiredColumnsFromCsv(basePath + "CAvideos.csv")
    val DEdf = readRequiredColumnsFromCsv(basePath + "DEvideos.csv")
    val FRdf = readRequiredColumnsFromCsv(basePath + "FRvideos.csv")
    val GBdf = readRequiredColumnsFromCsv(basePath + "GBvideos.csv")
    val INdf = readRequiredColumnsFromCsv(basePath + "INvideos.csv")
    val JPdf = readRequiredColumnsFromCsv(basePath + "JPvideos.csv")
    val KRdf = readRequiredColumnsFromCsv(basePath + "KRvideos.csv")
    val MXdf = readRequiredColumnsFromCsv(basePath + "MXvideos.csv")
    val RUdf = readRequiredColumnsFromCsv(basePath + "RUvideos.csv")
    val USdf = readRequiredColumnsFromCsv(basePath + "USvideos.csv")

    /*bigDf1: files with only string fields, but MXdf and RUdf have uppercase representations of boolean fields,
    * while the others have "True" and "False" representations*/
    val bigDf1 = CAdf.union(DEdf.union(FRdf.union(GBdf.union(USdf.union(MXdf.union(RUdf))))))
    val bigDf2 = INdf.union(JPdf.union(KRdf)) //files with some string fields and some boolean fields
    val bigDf = correctDf1(bigDf1).union(correctDf2(bigDf2))

    val outputDf = bigDf.coalesce(1)

    val outputFolder = "hdfs:/user/agnucci/datasets/youtubeDataset"
    FileSystem.get(sc.hadoopConfiguration).delete(new Path(outputFolder), true)
    outputDf.write.parquet(outputFolder)
  }

  //Creates a DataFrame from a path, keeping only the needed fields.
  def readRequiredColumnsFromCsv(path: String): DataFrame =
    spark.read
      .option("wholeFile", true).option("multiline",true) //supports multiline fields (fields with \n)
      .option("mode", "DROPMALFORMED") //ignores badly formed records
      .option("header", "true").option("delimiter", ",") //specifies csv header presence and delimiter
      .option("inferSchema", "true")
      .csv(path).drop("video_id","title","channel_title","category_id","views","likes","dislikes",
        "comment_count","thumbnail_link","comments_disabled","ratings_disabled","description")

  //Removes wrongly formatted rows and Converts the fields that should be boolean to boolean.
  def correctDf1(df: DataFrame): DataFrame =
    df.filter(filterRow(_, checkBooleanField = true))
      .withColumn("video_error_or_removed", $"video_error_or_removed".cast(BooleanType))

  //Removes wrongly formatted rows.
  def correctDf2(df: DataFrame): DataFrame =
    df.filter(filterRow(_, checkBooleanField = false))

  //Removes rows generated by the carriage return in the description fields.
  def filterRow(row: Row, checkBooleanField: Boolean): Boolean =
    checkDateFields(row) && (!checkBooleanField || checkVideoErrorOrRemoved(row))

  //Checks whether the Date fields are correctly formatted.
  def checkDateFields(row: Row): Boolean = {
    val trendingDate = row.getAs[String](0)
    val publishTime = row.getAs[String](1)
    try {
      new SimpleDateFormat(TRENDINGDATEFORMAT).parse(trendingDate)
      new SimpleDateFormat(PUBLISHTIMEFORMAT).parse(publishTime)
    } catch{
      case exception: ParseException => return false
    }
    true
  }

  //Checks if the field "video_error_or_removed" is correctly formatted.
  def checkVideoErrorOrRemoved(row: Row): Boolean = {
    val videoErrorOrRemoved = row.getAs[String](3).toLowerCase
    videoErrorOrRemoved == "true" || videoErrorOrRemoved == "false"
  }

}
